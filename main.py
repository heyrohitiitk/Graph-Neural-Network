# -*- coding: utf-8 -*-
"""dgl_spin_off_2.5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DiH-4Ow0WKy3VqA68bwraYaARlMXt3uX
"""

#from google.colab import drive
#drive.mount('/content/gdrive')
# Commented out IPython magic to ensure Python compatibility
# !git clone https://github.com/joerg84/Graph_Powered_ML_Workshop.git
# !rsync -av Graph_Powered_ML_Workshop/ ./ --exclude=.git
# !pip3 install dgl
# !pip3 install torch
# !pip3 install networkx

import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from dgl.nn import GraphConv
import dgl.function as fn
import math
from sklearn.model_selection import train_test_split
import os
import csv
from scipy.stats import rankdata
import argparse
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import pickle 
import _pickle as cPickle
import bz2
import time
from sklearn.metrics import classification_report
from sklearn.utils import shuffle

parser = argparse.ArgumentParser(description='Main function for FannieMae dataset')
parser.add_argument('--gen_graph', help="To generate graph from the train spectrums", action='store_true')
parser.add_argument('--train_gnn', help="To train GNN using generated graph", action='store_true')
parser.add_argument('--predict', help="To generate good nudges pass train arg", action='store_true')

args = parser.parse_args()

def tarantula(ef,ep,nf,np):
  if (ef+nf)==0:
      numer=0
  else:
      numer=ef/(ef+nf)
  if (ep+np)==0:
      denom=numer
  else:
      denom= numer + ep/(ep+np)
  if denom==0:
      score=0
  else:
      score=numer/denom
  return score

def ochiai2(ef,ep,nf,np):
  numer=ef*np
  denom=math.sqrt((ef+ep) * (nf+np) *(ef+np)* (ep+nf))
  if denom==0:
      score=0
  else:
      score=numer/denom
  return score

def overlap(ef,ep,nf,np):
  if min(ef,ep,nf) ==0:
      score=0
  else:
      score=ef/min(ef,ep,nf)
  return score

def ochiai(ef,ep,nf,np):
  denom=math.sqrt((ef+ep) * (ef+nf))
  if denom==0:
      score=0
  else:
      score=ef/denom
  return score

def barinel(ef,ep,nf,np):
  if (ep+ef) ==0:
      score=0
  else:
      score= 1 - (ep/(ep+ef))
  return score

def sbfl(ef,ep,nf,np):
  return [ochiai(ef,ep,nf,np),barinel(ef,ep,nf,np),overlap(ef,ep,nf,np),ochiai2(ef,ep,nf,np),tarantula(ef,ep,nf,np)]

def preprocess_spectrums(spect, total_T, total_C):
  T = spect.shape[0]
  C = spect.shape[1] - 1
  conn1,conn2 = np.nonzero(spect[:,:-1])
  conn1 += total_T
  conn2 += total_C
  conn1 = conn1.tolist()
  conn2 = conn2.tolist()
  T_embedding = np.zeros((T,5))
  T_embedding[np.where(spect[:,-1] ==1)[0],:] = np.ones(5)

  act = spect[:,:-1]
  err = (spect[:,-1]).reshape(-1,1)
  ef_arr = np.sum(act *(err),axis=0) #shape 1xT     
  ep_arr = np.sum(act *(1-err),axis=0) #shape 1xT
  nf_arr = np.sum((1-act)*err,axis=0) #shape 1xT
  np_arr = np.sum((1-act)*(1-err),axis=0) #shape 1xT
  C_embedding = np.zeros((C,5))
  for i in range(0,C):
    C_embedding[i] = sbfl(ef_arr[i],ep_arr[i],nf_arr[i],np_arr[i])  # Random Initialization
  return T_embedding.tolist(),C_embedding.tolist(), T, C,conn1,conn2


def create_graph(spectrums, path, i):
  total_T = total_C = 0
  totalT_embedding = []
  totalC_embedding = []
  num_C = []
  total_conn1 = []
  total_conn2 = []
  count = 0 
  for spectrum in spectrums:
    count += 1
    print('preprocessing spectrum number {}'.format(count))
    reader = list(csv.reader(open(path+spectrum, 'r'), delimiter=' '))
    spect = np.array(reader)
    spect[:, -1] = (spect[:, -1] == '1')*1
    spect = spect.astype(int)
    T_embedding,C_embedding, T, C,conn1,conn2 = preprocess_spectrums(spect, total_T, total_C)
    num_C.append(C)
    total_C = total_C + C
    total_T = total_T + T
    totalT_embedding = totalT_embedding + T_embedding
    totalC_embedding = totalC_embedding + C_embedding
    total_conn1 = total_conn1 + conn1
    total_conn2 = total_conn2 + conn2

  total_conn2 = np.array(total_conn2,dtype='int')
  total_conn2 += total_T
  total_conn2 = total_conn2.tolist()
  total_embedding = totalT_embedding + totalC_embedding

  temp = np.arange(total_C + total_T).tolist()
  g = dgl.graph((torch.tensor(total_conn1+total_conn2+temp),torch.tensor(total_conn2+total_conn1+temp)))

  g.ndata['h'] = torch.tensor(total_embedding)
  return g, total_T, total_C, num_C

def label(candidates, num_C, path):
  itr = 0
  C_label = []
  for candidate in candidates:
    reader = np.array(list(csv.reader(open(path+candidate, 'r')))).astype(int)
    
    C = num_C[itr]
    comp = np.array([0]*C)
    comp[(reader[0]-1,)] = 1  #changed code
    
    C_label = C_label+comp.tolist()
    itr = itr + 1
  C_label = torch.tensor(C_label)

  return C_label

class SAGEConv(nn.Module):
    def __init__(self, in_feat, out_feat):
        super(SAGEConv, self).__init__()
        self.linear = nn.Linear(in_feat * 2, out_feat)

    def forward(self, g, h, T,C):
      '''
        g  : Graph
            The input graph.
        h : Tensor
            The input node feature.
      '''
      with g.local_scope():
          g.ndata['h'] = h
          
          num_test = np.arange(T)
          num_comp = (np.arange(C) + T)
          g.prop_nodes([num_comp,num_test,num_comp],message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N'))
          h_N = g.ndata['h_N']
          h_total = torch.cat([h, h_N], dim=1)
          
          return self.linear(h_total)

class MLP(nn.Module):
        def __init__(self, input_size, hidden_size):
            super(MLP, self).__init__()
            self.input_size = input_size
            self.hidden_size  = hidden_size
            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)
            self.relu = torch.nn.ReLU()
            self.fc2 = torch.nn.Linear(self.hidden_size, 1)
            self.sigmoid = torch.nn.Sigmoid()
            self.dropout = torch.nn.Dropout(p=0.1, inplace=False)
            self.tanh = torch.nn.Tanh()

        def forward(self, feat):
            hidden1 = self.fc1(feat)
            hidden1 = self.relu(hidden1)
            hidden1 = self.dropout(hidden1)
            output = self.fc2(hidden1)
            #output = self.relu(output)
            output = self.sigmoid(output)
            return output

class Model(nn.Module):
    def __init__(self, in_feats, h_feats, out_feats):
        super(Model, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats)
        self.feed = MLP(h_feats, h_feats)

    def forward(self, g, in_feat, T, C):
        h = self.conv1(g, in_feat, T, C)
        h = torch.sigmoid(h)
        h = self.feed(h)
        return h

def focal_loss(logits, labels, alpha, y):
  alpha1 = (alpha)*torch.pow((1-logits), y)
  alpha2 = (1-alpha)*torch.pow(logits, y)
  labels = np.reshape(labels, (labels.shape[0], 1))
  alphat = alpha1*(labels) + alpha2*(1-labels)
  l1 = -torch.log(1-logits) #non buggy
  l2 = -torch.log(logits) #buggy
  loss = l1.mul(1 - labels) + l2.mul(labels)
  #print(loss.shape)
  total_loss = loss.mul(alphat)
  #print(total_loss.shape)
  return torch.mean(total_loss)

def acc(p, act):
  p = p.reshape(-1,1)
  act = act.reshape(-1,1)
  p[p < 0.5] = 0
  p[p >= 0.5] = 1
  
  TP = torch.sum(p.mul(act))
  FP = torch.sum((1-act).mul(p)) #predicting non buggy as buggy
  FN = torch.sum((act.mul(1-p))) #predicting buggy as non buggy
  TN = torch.sum(((1-act).mul(1-p)))

  precision = TP / (TP + FP) #low => predicting non buggy as buggy
  recall = TP / (TP + FN) #low => predicting buggy as non buggy

  print('precision: {:.4f}, recall: {:.4f}, TP: {}, FP: {}, FN: {}, TN: {}'.format(precision, recall, TP,FP,FN,TN))

def accuracy(pred,act,C, n):
    #precision = sum(if pred > 1)/
    pred = 1 - pred
    pred = pred.detach().numpy()
    rank = rankdata(pred, method='min')
    topn = n/100 * C
    cntn = 0

    act = act.detach().numpy()
    
    sorted_ind = np.argsort(rank)

    cntn = np.sum(act[(sorted_ind,)][:int(topn)+1])   #changed code

    return cntn
    #print('comps={}, bugs={}, {:.1f}% bugs in top {:.0f}%'.format(C, total_bugs, cntn/total_bugs * 100, n*100))

def train(g, model, alpha, y):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    features = g.ndata['h']
    labels = g.ndata['label']
    train_mask = g.ndata['train_mask']
    validation_mask = g.ndata['validation_mask']
    epochs = 300
    for e in range(epochs):
        # Forward
        logits = model(g, features, total_T, total_C)
        
        loss = focal_loss(logits[train_mask], labels[train_mask], alpha, y)

        # Backward
        optimizer.zero_grad()
        loss.backward()        
        optimizer.step()
        if e % 100 == 0:
            print('In epoch {}, loss: {:.3f}'.format(e, loss))
            with open('training.out','a+') as file:
              writer = csv.writer(file)
              writer.writerow(['In epoch {}, loss: {:.3f}'.format(e, loss)])
        if e == epochs - 1:
          return logits

#global variables declaration
train_path = './data_synthetic_spectrums/'
test_path = './data_real/'

if args.gen_graph:
  files = os.listdir(train_path)
  spectrums = []
  candidates = []
  itr = 0
  for fname in files:
    if 'Spectrum' in fname:
      spectrums.append(fname)
      candidates.append(fname.replace('Spectrum', 'Candidates'))
      itr = itr + 1
      if itr>=100:
        break
  spectrums, candidates = shuffle(spectrums, candidates)
  '''
  temp_spectrums = []
  temp_candidates = []
  for s,c in zip(spectrums, candidates):
    if s.split('_')[1] == '[0, 0, 1]':
      temp_spectrums.append(s)
      temp_candidates.append(c)
  
  spectrums = temp_spectrums
  candidates = temp_candidates
  spectrums, test_spectrums, candidates, test_candidates = train_test_split(spectrums, candidates, test_size=0.2, shuffle=False,random_state=42)
  '''
  pickle.dump(spectrums,open('./pickles/spectrums','wb'))
  #pickle.dump(test_spectrums,open('./pickles/test_spectrums','wb'))
  pickle.dump(candidates,open('./pickles/candidates','wb'))
  #pickle.dump(test_candidates,open('./pickles/test_candidates','wb'))

  print('starting to create giant graph using train-spectrums....')
  start = time.time()
  g, total_T, total_C, num_C = create_graph(spectrums, train_path, 0)
  print('graph creation is done. adding training and validation masks')
  g.ndata['label'] = torch.cat([torch.ones((total_T,)),label(candidates,num_C, train_path)],dim=0)

  g.ndata['train_mask'] = torch.zeros(total_T+total_C, dtype=torch.bool)
  g.ndata['validation_mask'] = torch.zeros(total_T+total_C, dtype=torch.bool)

  g.ndata['train_mask'][total_T:total_T+total_C] = True

  spec_sz = (int)(len(spectrums)/10)
  labels = g.ndata['label']
  end = time.time()
  print('time taken to create graph is now {} seconds'.format(end-start))

  #dump the graph so that it can be used later to train GNN
  print('dumping generated graph and intermidiate data..')
  pickle.dump(g,open('./pickles/train_graph.pkl','wb'))
  with bz2.BZ2File('./pickles/train_graph.pbz2', 'wb') as f: 
    cPickle.dump(g, f)
  pickle.dump(total_T,open('./pickles/total_T','wb'))
  pickle.dump(total_C,open('./pickles/total_C','wb'))
  pickle.dump(num_C,open('./pickles/num_C','wb'))
  print(os)

if args.train_gnn:
  #load data
  
  spectrums = pickle.load(open('./pickles/spectrums','rb'))
  #test_spectrums = pickle.load(open('./pickles/test_spectrums','rb'))
  candidates = pickle.load(open('./pickles/candidates','rb'))
  #test_candidates = pickle.load(open('./pickles/test_candidates','rb'))
  
  
  g = bz2.BZ2File('./pickles/train_graph.pbz2', 'rb')
  g = cPickle.load(g)
  total_T = pickle.load(open('./pickles/total_T','rb'))
  total_C = pickle.load(open('./pickles/total_C','rb'))
  num_C = pickle.load(open('./pickles/num_C','rb'))

  spec_sz = (int)(len(spectrums)/10)
  labels = g.ndata['label']
  print('fold size :',spec_sz)

  results = {}
  #defining model here
  model = Model(g.ndata['h'].shape[1], g.ndata['h'].shape[1], 1)
  #model = Model(g.ndata['h'].shape[1],20, 1)
  #print(model.eval())
  #model.train()

  file = open('training.out','w')
  file.close()
  #K-Fold Validation
  for k in range(10):
    print('Fold number {}'.format(k+1))
    console_out = 'Fold number {}'.format(k+1)
    with open('training.out','a+') as file:
      writer = csv.writer(file)
      writer.writerow([console_out])
    k1 = (int)(np.sum(num_C[:k*spec_sz])) # starting index
    k2 = (int)(np.sum(num_C[:k*spec_sz + spec_sz])) #end index : bug fixed
    g.ndata['train_mask'][total_T + k1: total_T + k2] = False #masking for components note : not fot test cases.
    start = time.time()
    logits = train(g, model, 0.978, 4)
    end = time.time()
    print('training time : ',end - start)
    ac = []
    for itr in range(spec_sz): # kth fold itrth spetrum : bug fixed
      j1 = (int)(np.sum(num_C[:k*spec_sz + itr]))
      j2 = (int)(np.sum(num_C[:k*spec_sz + itr+1]))
      g.ndata['validation_mask'][total_T + j1: total_T + j2] = True

      temp = {}
      total_bugs = np.sum(labels[g.ndata['validation_mask']].detach().numpy())
      for n in [5]:#(5,10,20,50):
        cntn = accuracy(logits[g.ndata['validation_mask']],labels[g.ndata['validation_mask']], j2 - j1, n)
        temp[n] = cntn*100/total_bugs
      ac.append(temp[5])
      results[spectrums[k*spec_sz+itr]] = [j2-j1, total_bugs, temp] 

      g.ndata['validation_mask'][total_T + j1: total_T + j2] = False #bug fixed
    g.ndata['train_mask'][total_T + k1: total_T + k2] = True #resorting kth folder training to true

    g.ndata['validation_mask'][total_T + k1: total_T + k2] = True
    acc(logits[g.ndata['validation_mask']],labels[g.ndata['validation_mask']])
    g.ndata['validation_mask'][total_T + k1: total_T + k2] = False
    print('median top 5%',np.median(ac))
  pickle.dump(model,open('model_relu','wb'))###model_
  #print(results)


if args.predict:
  files = os.listdir(test_path)
  test_spectrums = []
  test_candidates = []
  for fname in files:
    if 'Spectrum' in fname:
      test_spectrums.append(fname)
      test_candidates.append(fname.replace('Spectrum', 'Candidates'))
  
  model = pickle.load(open('model_relu','rb'))
  #spectrums = pickle.load(open('./pickles/spectrums','rb'))
  #test_spectrums = pickle.load(open('./pickles/test_spectrums','rb'))
  #candidates = pickle.load(open('./pickles/candidates','rb'))
  #test_candidates = pickle.load(open('./pickles/test_candidates','rb'))
 
  wasted_effort_ochiai = {}
  wasted_effort_gnn = {}

  results_test = {}
  WE_tanh = []
  for i in range(len(test_spectrums)):
    g_test,T_test,C_test,_ = create_graph([test_spectrums[i]], test_path, i)
    labels = label([test_candidates[i]],[C_test], test_path)  
    
    logits_och = g_test.ndata['h'][:,0]
    logits_gnn = model(g_test, g_test.ndata['h'],T_test,C_test)
    print('logits_gnn: ', i, logits_gnn)
    pred_och = (1-logits_och[T_test:T_test+C_test]).detach().numpy()
    pred_gnn = (1-logits_gnn[T_test:T_test+C_test]).detach().numpy()

    rank_och = rankdata(pred_och, method='min')
    rank_gnn = rankdata(pred_gnn, method='min')
    #print('rank_gnn: ', i, np.unique(rank_gnn))
    act = labels.detach().numpy()  
    
    temp_och = []
    temp_gnn = []
    
    for j in range(C_test):
      if act[j]==1:
        temp_och.append((rank_och[j]-1)/(C_test-1))
        temp_gnn.append((rank_gnn[j]-1)/(C_test-1))
        WE_tanh.append((rank_gnn[j]-1)/(C_test-1))

    ###wasted effort
    wasted_effort_ochiai[test_spectrums[i]] = np.median(np.array(temp_och))
    wasted_effort_gnn[test_spectrums[i]] = np.median(np.array(temp_gnn))

    ###topn
    temp = {}
    total_bugs = np.sum(labels.detach().numpy())
    for n in (5,10,20,50):
      cntn = accuracy(logits_gnn[T_test:T_test+C_test],labels,C_test, n)
      temp[n] = cntn*100/total_bugs
    results_test[test_spectrums[i]] = [C_test, total_bugs, temp]

  ###WE_relu
  #pickle.dump(WE_relu,open('WE_tanh','wb'))  
  #print(wasted_effort_gnn)

  spectnames = []
  we_gnn = []
  we_ochiai = []

  for i in wasted_effort_gnn:
    spectnames.append(i)
    we_gnn.append(wasted_effort_gnn[i])
    we_ochiai.append(wasted_effort_ochiai[i])
    #print('wasted effort', i, wasted_effort_gnn[i], wasted_effort_ochiai[i])
  print('wasted effort', np.sum(we_gnn), np.sum(we_ochiai))
  
  plt.rcParams['figure.figsize'] = [24, 8]

  X_axis = np.arange(len(spectnames))
    
  plt.bar(X_axis - 0.15, we_gnn, 0.3, label = 'gnn')
  plt.bar(X_axis + 0.15, we_ochiai, 0.3, label = 'ochiai')
    
  plt.xticks(X_axis, spectnames)
  plt.xlabel("spectrums")
  plt.ylabel("Median Wasted Effort")
  plt.xticks(rotation=90)
  plt.title('Wasted Effort of different spectrums')
  plt.legend()
  plt.show()
  #print(1/0)
  spectnames = []
  top5 = []
  top10 = []
  top20 = []
  top50 = []
  for i in sorted(results_test):
    spectnames.append(i)
    val = results_test[i]
    top5.append(val[2][5])
    top10.append(val[2][10])
    top20.append(val[2][20])
    top50.append(val[2][50])

  print(len(top5), len(spectnames))
  plt.rcParams['figure.figsize'] = [16, 8]
  plt.bar(spectnames, top50, label="top 50%", color='black')
  plt.bar(spectnames, top20, label="top 20%", color='r')
  plt.bar(spectnames, top10, label="top 10%", color='g')
  plt.bar(spectnames, top5, label="top 5%", color='yellow')

  plt.xlabel("spectrums")
  plt.ylabel("% of bugs captured")
  plt.xticks(rotation=90)
  plt.legend()
  plt.title('Ranks according to GNN by SBFL Initialization')
  plt.savefig('test_gnn_sbfl_init.png',format="png", bbox_inches='tight')
  plt.show()
  #print(results_test['Chart_bug18_cov1_Spectrum.txt'])

  #####################
  ochiai_test = {}
  for i in range(len(test_spectrums)):
    g_test,T_test,C_test,_ = create_graph([test_spectrums[i]],test_path,i)
    labels = label([test_candidates[i]],[C_test],test_path)

    logits = g_test.ndata['h'][:,0]

    temp = {}
    total_bugs = np.sum(labels.detach().numpy())
    for n in (5,10,20,50):
      cntn = accuracy(logits[T_test:T_test+C_test],labels,C_test, n)
      temp[n] = cntn*100/total_bugs
    ochiai_test[test_spectrums[i]] = [C_test, total_bugs, temp]
  print(ochiai_test)

  spectnames = []
  top5 = []
  top10 = []
  top20 = []
  top50 = []
  for i in sorted(ochiai_test):
    spectnames.append(i)
    val = ochiai_test[i]
    top5.append(val[2][5])
    top10.append(val[2][10])
    top20.append(val[2][20])
    top50.append(val[2][50])

  print(len(top5), len(spectnames))
  plt.rcParams['figure.figsize'] = [16, 8]
  plt.bar(spectnames, top50, label="top 50%", color='black')
  plt.bar(spectnames, top20, label="top 20%", color='r')
  plt.bar(spectnames, top10, label="top 10%", color='g')
  plt.bar(spectnames, top5, label="top 5%", color='yellow')

  plt.xlabel("spectrums")
  plt.ylabel("% of bugs captured")
  plt.xticks(rotation=90)
  plt.legend()
  plt.title('Ranks according to Ochiai scores')
  plt.savefig('test_ochiai.png',format="png", bbox_inches='tight')
  plt.show()

  '''
  print(results_test)

  import json 
  with open("validation_results.json", "w") as outfile:
      json.dump(str(results), outfile)

  with open("test_results.json", "w") as outfile:
      json.dump(str(results_test), outfile)
  '''
  for perc in (5,):
  
    percentage_improv = []
    for name in sorted(test_spectrums):
      percentage_improv.append(results_test[name][2][perc] - ochiai_test[name][2][perc])

    positive_improv = []
    negative_improv = []
    for val in percentage_improv:
      if val>=0:
        positive_improv.append(val)
        negative_improv.append(0)
      else:
        negative_improv.append(val)
        positive_improv.append(0)


    plt.xlabel("test_spectrums")
    plt.ylabel("% improvement")
    plt.axhline(y=0)
    plt.legend()
    plt.title('Percentage Improvement in {}%'.format(perc))
    plt.xticks(rotation=90)
    plt.bar(sorted(test_spectrums), percentage_improv, color='g')
    #plt.bar(sorted(test_spectrums), positive_improv, color='g')
    #plt.bar(sorted(test_spectrums), negative_improv, color='r')
    plt.savefig('perc_improv_{}.png'.format(i),format="png", bbox_inches='tight')
